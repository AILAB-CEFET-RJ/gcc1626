{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Redes Neurais Recorrentes (RNN) e LSTMs — PyTorch Puro\n","\n","Este notebook ilustra o funcionamento das redes recorrentes:\n","- Como o estado oculto é atualizado;\n","- Como o PyTorch implementa `nn.RNN` e `nn.LSTM`;\n","- E como elas aprendem dependências temporais em sequências."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Recorrência passo a passo\n","\n","O estado oculto `h_t` é atualizado a cada passo de tempo:\n","\n","$$\n","h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n","$$\n","\n","onde `f` é uma função de ativação não linear (ex: `tanh`)."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estado inicial: tensor([0., 0., 0.])\n","h_1 = tensor([ 0.6291,  0.8989, -0.8216])\n","h_2 = tensor([-0.6662, -0.6302,  0.7730])\n","h_3 = tensor([-0.0694, -0.9242,  0.8592])\n"]}],"source":["torch.manual_seed(0)\n","\n","# Vetores de entrada (3 passos, cada um de dimensão 2)\n","x_seq = [torch.randn(2) for _ in range(3)]\n","\n","# Parâmetros manuais\n","W_xh = torch.randn(2, 3)\n","W_hh = torch.randn(3, 3)\n","b_h = torch.zeros(3)\n","\n","h = torch.zeros(3)\n","print(\"Estado inicial:\", h)\n","\n","for i, x_t in enumerate(x_seq):\n","    h = torch.tanh(x_t @ W_xh + h @ W_hh + b_h)\n","    print(f\"h_{i+1} =\", h)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Usando `nn.RNN` no PyTorch\n","\n","O módulo `nn.RNN` faz automaticamente o mesmo processo."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saída (por passo): tensor([[[-0.8324,  0.7152,  0.1593],\n","         [ 0.2347, -0.0078,  0.5524],\n","         [ 0.4113, -0.5408,  0.5808]]], grad_fn=<TransposeBackward1>)\n","Estado final: tensor([[[ 0.4113, -0.5408,  0.5808]]], grad_fn=<StackBackward0>)\n"]}],"source":["rnn = nn.RNN(input_size=2, hidden_size=3, batch_first=True)\n","\n","# Uma sequência com batch_size=1 e seq_len=3\n","x = torch.stack(x_seq).unsqueeze(0)\n","out, h_final = rnn(x)\n","\n","print(\"Saída (por passo):\", out)\n","print(\"Estado final:\", h_final)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Treinando uma RNN simples\n","\n","Vamos criar uma tarefa de previsão de próxima letra em uma sequência curta.\n","O objetivo é prever a próxima letra da palavra \"ola\"."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Época 5 | Loss: 0.1912\n","Época 10 | Loss: 0.0106\n","Época 15 | Loss: 0.0017\n","Época 20 | Loss: 0.0006\n","Época 25 | Loss: 0.0003\n","Época 30 | Loss: 0.0002\n","Época 35 | Loss: 0.0001\n","Época 40 | Loss: 0.0001\n","Época 45 | Loss: 0.0001\n","Época 50 | Loss: 0.0001\n","Entrada: 'ol' → Previsão: 'a'\n"]}],"source":["# mapeia caracteres\n","char_to_idx = {\"o\": 0, \"l\": 1, \"a\": 2}\n","idx_to_char = {i: c for c, i in char_to_idx.items()}\n","\n","seq = [0, 1, 2]  # o, l, a\n","x_data = torch.tensor([[0, 1]], dtype=torch.long)  # \"ol\"\n","y_data = torch.tensor([2], dtype=torch.long)       # \"a\"\n","\n","# Modelo simples\n","class CharRNN(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x):\n","        emb = self.emb(x)\n","        out, _ = self.rnn(emb)\n","        out = self.fc(out[:, -1, :])  # usa último passo\n","        return out\n","\n","model = CharRNN(vocab_size=3, hidden_size=8)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n","\n","for epoch in range(50):\n","    optimizer.zero_grad()\n","    output = model(x_data)\n","    loss = criterion(output, y_data)\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch+1) % 5 == 0:\n","        print(f\"Época {epoch+1} | Loss: {loss.item():.4f}\")\n","\n","pred = model(x_data).argmax(dim=1).item()\n","print(f\"Entrada: 'ol' → Previsão: '{idx_to_char[pred]}'\")"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Usando `nn.LSTM`\n","\n","A LSTM (Long Short-Term Memory) introduz *portas* que controlam o fluxo de informação.\n","Ela é mais estável que a RNN em sequências longas."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saída (LSTM): tensor([[[ 0.1721, -0.0125, -0.1078],\n","         [ 0.2675,  0.1160, -0.2038],\n","         [ 0.4472,  0.2182, -0.3425]]], grad_fn=<TransposeBackward0>)\n","Estado oculto final: tensor([[[ 0.4472,  0.2182, -0.3425]]], grad_fn=<StackBackward0>)\n","Estado de célula (memória interna): tensor([[[ 0.6073,  0.5507, -0.6291]]], grad_fn=<StackBackward0>)\n"]}],"source":["lstm = nn.LSTM(input_size=2, hidden_size=3, batch_first=True)\n","x = torch.stack(x_seq).unsqueeze(0)\n","out, (h_n, c_n) = lstm(x)\n","\n","print(\"Saída (LSTM):\", out)\n","print(\"Estado oculto final:\", h_n)\n","print(\"Estado de célula (memória interna):\", c_n)"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Comparação RNN vs LSTM\n","\n","- **RNN:** propaga apenas o estado oculto `h_t`; tende a sofrer com *vanishing gradients*.\n","- **LSTM:** mantém também um estado de célula `c_t`, que preserva melhor informações de longo prazo.\n","\n","LSTMs são preferidas na maioria das tarefas de sequência mais longas (tradução, fala, etc.).\n","\n","---\n","\n","> **Resumo:** RNNs capturam dependências locais em sequências; LSTMs ampliam essa capacidade com memória controlada por portas."]}],"metadata":{"kernelspec":{"display_name":"gcc1626","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":5}
