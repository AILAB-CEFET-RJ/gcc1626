{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Entendendo a classe `nn.Embedding` (PyTorch)\n","\n","A classe `nn.Embedding` é usada para mapear **índices inteiros para vetores densos aprendíveis**.\n","Ela é amplamente utilizada em Processamento de Linguagem Natural (PLN), sistemas de recomendação e redes neurais que recebem dados categóricos.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Definição\n","\n","```python\n","torch.nn.Embedding(num_embeddings, embedding_dim)\n","```\n","\n","- **`num_embeddings`**: número de tokens no vocabulário (tamanho total da tabela);\n","- **`embedding_dim`**: dimensão do vetor denso associado a cada token.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Funcionamento interno\n","\n","A camada mantém uma matriz de pesos `W` de dimensão `(num_embeddings, embedding_dim)`.\n","Cada linha representa o vetor correspondente a um token. Quando o modelo recebe um índice, ele retorna a linha correspondente da matriz.\n","\n","Em outras palavras:\n","\n","$$\n","x = [i_1, i_2, \\ldots, i_n] \\quad \\Rightarrow \\quad \\text{Embedding}(x) = [W_{i_1}, W_{i_2}, \\ldots, W_{i_n}]\n","$$\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.8068,  1.0841,  0.4338,  0.6632,  0.7278,  1.1392],\n","        [ 0.4861, -1.8468, -0.1480,  1.7970,  1.2261, -1.4240],\n","        [ 1.4029, -1.0909, -0.1549, -0.1321, -2.7201, -0.5939],\n","        [ 0.0324, -0.5573,  1.3662, -1.0282,  0.4197,  1.3492],\n","        [-0.5959,  0.8269, -0.4199,  0.0620, -0.0121,  1.6231]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","# vocabulário com 5 palavras, cada uma com vetor de dimensão 3\n","emb = nn.Embedding(num_embeddings=50, embedding_dim=6)\n","\n","# índices dos tokens: [0, 3, 4]\n","x = torch.tensor([0, 3, 4, 12, 10])\n","print(emb(x))"]},{"cell_type":"markdown","metadata":{},"source":["A saída é uma matriz onde cada linha é o vetor correspondente ao índice fornecido."]},{"cell_type":"markdown","metadata":{},"source":["## Durante o treinamento\n","\n","- Os vetores de embeddings são **parâmetros aprendíveis**.\n","- O gradiente flui de volta até a matriz de embeddings durante o `backpropagation`.\n","- O modelo ajusta os vetores para que palavras usadas em contextos semelhantes fiquem próximas no espaço vetorial.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Comparação intuitiva\n","\n","| Tipo de representação | Exemplo                     | Dimensão | Semântica |\n","|------------------------|-----------------------------|-----------|------------|\n","| One-hot               | `[0, 0, 1, 0, 0]`           | |V|       | Nenhuma    |\n","| Embedding (learned)   | `[0.34, -0.27, 0.10, ...]`  | d (ex: 50)| Captura contextos e semelhanças |\n"]},{"cell_type":"markdown","metadata":{},"source":["## Usos típicos\n","- Representação de palavras em PLN;\n","- Representação de itens em sistemas de recomendação;\n","- Codificação de variáveis categóricas em redes neurais tabulares."]}],"metadata":{"kernelspec":{"display_name":"gcc1626","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":5}
