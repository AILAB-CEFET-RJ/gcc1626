{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Entendendo a classe `nn.Embedding` (PyTorch)\n", "\n", "A classe `nn.Embedding` \u00e9 usada para mapear **\u00edndices inteiros para vetores densos aprend\u00edveis**.\n", "Ela \u00e9 amplamente utilizada em Processamento de Linguagem Natural (PLN), sistemas de recomenda\u00e7\u00e3o e redes neurais que recebem dados categ\u00f3ricos.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Defini\u00e7\u00e3o\n", "\n", "```python\n", "torch.nn.Embedding(num_embeddings, embedding_dim)\n", "```\n", "\n", "- **`num_embeddings`**: n\u00famero de tokens no vocabul\u00e1rio (tamanho total da tabela);\n", "- **`embedding_dim`**: dimens\u00e3o do vetor denso associado a cada token.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Funcionamento interno\n", "\n", "A camada mant\u00e9m uma matriz de pesos `W` de dimens\u00e3o `(num_embeddings, embedding_dim)`.\n", "Cada linha representa o vetor correspondente a um token. Quando o modelo recebe um \u00edndice, ele retorna a linha correspondente da matriz.\n", "\n", "Em outras palavras:\n", "\n", "\\[\n", "x = [i_1, i_2, \\ldots, i_n] \\quad \\Rightarrow \\quad \\text{Embedding}(x) = [W_{i_1}, W_{i_2}, \\ldots, W_{i_n}]\n", "\\]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "\n", "# vocabul\u00e1rio com 5 palavras, cada uma com vetor de dimens\u00e3o 3\n", "emb = nn.Embedding(num_embeddings=5, embedding_dim=3)\n", "\n", "# \u00edndices dos tokens: [0, 3, 4]\n", "x = torch.tensor([0, 3, 4])\n", "print(emb(x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A sa\u00edda \u00e9 uma matriz onde cada linha \u00e9 o vetor correspondente ao \u00edndice fornecido."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Durante o treinamento\n", "\n", "- Os vetores de embeddings s\u00e3o **par\u00e2metros aprend\u00edveis**.\n", "- O gradiente flui de volta at\u00e9 a matriz de embeddings durante o `backpropagation`.\n", "- O modelo ajusta os vetores para que palavras usadas em contextos semelhantes fiquem pr\u00f3ximas no espa\u00e7o vetorial.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compara\u00e7\u00e3o intuitiva\n", "\n", "| Tipo de representa\u00e7\u00e3o | Exemplo                     | Dimens\u00e3o | Sem\u00e2ntica |\n", "|------------------------|-----------------------------|-----------|------------|\n", "| One-hot               | `[0, 0, 1, 0, 0]`           | |V|       | Nenhuma    |\n", "| Embedding (learned)   | `[0.34, -0.27, 0.10, ...]`  | d (ex: 50)| Captura contextos e semelhan\u00e7as |\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Usos t\u00edpicos\n", "- Representa\u00e7\u00e3o de palavras em PLN;\n", "- Representa\u00e7\u00e3o de itens em sistemas de recomenda\u00e7\u00e3o;\n", "- Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas em redes neurais tabulares."]}], "metadata": {"language_info": {"name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}