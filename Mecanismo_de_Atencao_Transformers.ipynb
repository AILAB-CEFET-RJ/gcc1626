{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Mecanismo de Aten\u00e7\u00e3o e o Surgimento dos Transformers (PyTorch puro)\n", "\n", "Este notebook demonstra o funcionamento b\u00e1sico do mecanismo de **aten\u00e7\u00e3o**, n\u00facleo da arquitetura Transformer proposta por Vaswani et al. (2017).\n", "\n", "Etapas:\n", "1. Entender o c\u00e1lculo da aten\u00e7\u00e3o escalar\n", "2. Estender para vetores de *queries*, *keys* e *values*\n", "3. Implementar *self-attention*\n", "4. Visualizar pesos de aten\u00e7\u00e3o\n", "5. Mostrar o efeito de m\u00faltiplas cabe\u00e7as de aten\u00e7\u00e3o"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Aten\u00e7\u00e3o como produto escalar\n", "\n", "A aten\u00e7\u00e3o mede a similaridade entre uma *query* e um conjunto de *keys*.\n", "O peso de cada elemento \u00e9 proporcional ao produto escalar entre a *query* e a *key*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query = torch.tensor([1.0, 0.0])\n", "keys = torch.tensor([[1.0, 0.0],\n", "                     [0.7, 0.7],\n", "                     [0.0, 1.0]])\n", "\n", "scores = torch.matmul(keys, query)\n", "weights = F.softmax(scores, dim=0)\n", "\n", "print(\"Scores:\", scores)\n", "print(\"Pesos softmax:\", weights)\n", "\n", "plt.bar([\"k1\", \"k2\", \"k3\"], weights.numpy())\n", "plt.title(\"Pesos de Aten\u00e7\u00e3o (Softmax)\")\n", "plt.ylabel(\"Intensidade\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Aten\u00e7\u00e3o vetorial (Q, K, V)\n", "\n", "A sa\u00edda \u00e9 uma m\u00e9dia ponderada dos *values*, onde os pesos v\u00eam da similaridade\n", "entre *queries* e *keys*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Q = torch.tensor([[1.0, 0.0]])\n", "K = torch.tensor([[1.0, 0.0],\n", "                  [0.7, 0.7],\n", "                  [0.0, 1.0]])\n", "V = torch.tensor([[1.0, 2.0],\n", "                  [0.5, 1.0],\n", "                  [0.0, 3.0]])\n", "\n", "scores = torch.matmul(Q, K.T) / np.sqrt(K.shape[1])\n", "weights = F.softmax(scores, dim=1)\n", "att_output = torch.matmul(weights, V)\n", "\n", "print(\"Scores:\", scores)\n", "print(\"Pesos:\", weights)\n", "print(\"Sa\u00edda de aten\u00e7\u00e3o:\", att_output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Self-Attention em uma sequ\u00eancia\n", "\n", "Agora, cada posi\u00e7\u00e3o da sequ\u00eancia atua como *query*, *key* e *value* simultaneamente."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def self_attention(X):\n", "    d = X.shape[-1]\n", "    scores = torch.matmul(X, X.T) / np.sqrt(d)\n", "    weights = F.softmax(scores, dim=1)\n", "    out = torch.matmul(weights, X)\n", "    return out, weights\n", "\n", "X = torch.randn(3, 4)\n", "out, weights = self_attention(X)\n", "\n", "print(\"Pesos de aten\u00e7\u00e3o:\\n\", weights)\n", "print(\"Sa\u00edda:\\n\", out)\n", "\n", "plt.imshow(weights.detach().numpy(), cmap=\"Blues\")\n", "plt.title(\"Mapa de Aten\u00e7\u00e3o (Self-Attention)\")\n", "plt.xlabel(\"Keys\")\n", "plt.ylabel(\"Queries\")\n", "plt.colorbar()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Multi-Head Attention\n", "\n", "O Transformer usa m\u00falticas cabe\u00e7as de aten\u00e7\u00e3o para capturar diferentes padr\u00f5es\n", "de relacionamento entre tokens."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MultiHeadAttention:\n", "    def __init__(self, d_model, num_heads):\n", "        assert d_model % num_heads == 0\n", "        self.d_k = d_model // num_heads\n", "        self.num_heads = num_heads\n", "        self.Wq = torch.randn(num_heads, d_model, self.d_k)\n", "        self.Wk = torch.randn(num_heads, d_model, self.d_k)\n", "        self.Wv = torch.randn(num_heads, d_model, self.d_k)\n", "\n", "    def __call__(self, X):\n", "        heads = []\n", "        for i in range(self.num_heads):\n", "            Q = X @ self.Wq[i]\n", "            K = X @ self.Wk[i]\n", "            V = X @ self.Wv[i]\n", "            scores = Q @ K.T / np.sqrt(self.d_k)\n", "            weights = F.softmax(scores, dim=-1)\n", "            head = weights @ V\n", "            heads.append(head)\n", "        return torch.cat(heads, dim=-1)\n", "\n", "X = torch.randn(5, 8)\n", "mha = MultiHeadAttention(d_model=8, num_heads=2)\n", "out = mha(X)\n", "print(\"Sa\u00edda (multi-head):\", out.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Compara\u00e7\u00e3o: m\u00e9dia simples vs. aten\u00e7\u00e3o\n", "\n", "Enquanto a m\u00e9dia simples trata todos os tokens igualmente, a aten\u00e7\u00e3o aprende\n", "a focar nos elementos mais relevantes do contexto."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def simple_mean(X):\n", "    return X.mean(dim=0)\n", "\n", "X = torch.tensor([[1.0, 2.0],\n", "                  [0.0, 0.0],\n", "                  [5.0, 1.0]])\n", "\n", "mean_vec = simple_mean(X)\n", "att_vec, att_w = self_attention(X)\n", "\n", "print(\"M\u00e9dia simples:\", mean_vec)\n", "print(\"Self-Attention (ponderada):\", att_vec)\n", "print(\"Pesos de aten\u00e7\u00e3o:\\n\", att_w)"]}], "metadata": {"language_info": {"name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}